[
  {
    "objectID": "pages/600-exercises.html",
    "href": "pages/600-exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Try these exercises to practice what you’ve learned:\n\nExtract all URLs from a text document using regex\nCreate a function to normalize company names for deduplication\nImplement a simple spell checker using fuzzy matching and a dictionary\nWrite a regex to validate password strength based on common criteria\nCreate a function that can extract structured data (like addresses) from unstructured text",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "pages/400-combined-techniques.html",
    "href": "pages/400-combined-techniques.html",
    "title": "Combined Techniques: Putting It All Together",
    "section": "",
    "text": "Let’s solve a practical problem that combines strings, regex, and fuzzy matching.\nProblem Statement: We have a dataset of product descriptions that may refer to the same product but with different wording or formatting. We need to:\n\nClean and normalize the product descriptions\nGroup similar products together\n\nApproach:\n\nAnalyze the data: Notice what varies between similar product descriptions (order of words, extra information in parentheses, spacing around storage values)\nNormalize the text: Create a standard format to make comparison easier\nCompare the normalized descriptions: Use fuzzy matching to find similar products\nGroup the results: Place similar products together\n\n\nStep 1: Understand and Normalize the Data\n\n# Scenario: Cleaning and matching product descriptions\nimport re\nfrom fuzzywuzzy import fuzz\n\n# Sample data: Product descriptions\nproducts = [\n    \"Samsung Galaxy S21 5G 128GB Smartphone (Unlocked)\",\n    \"Samsung Galaxy S21 Ultra 256GB 5G Phone\",\n    \"Samsung S21 5G 128 GB\",\n    \"iPhone 13 Pro Max 256GB - Graphite\",\n    \"Apple iPhone 13 Pro Max (256 GB, Graphite)\",\n    \"iPhone13 Pro 256GB\",\n    \"Google Pixel 6 Pro 128GB - Stormy Black\",\n    \"Pixel 6 Pro (128 GB) Stormy Black color\"\n]\n\n\ndef normalize_product_text(text):\n    \"\"\"Clean and normalize product text for better matching\"\"\"\n    # Step 1: Convert to lowercase for case-insensitive comparison\n    text = text.lower()\n    \n    # Step 2: Remove parentheses and their contents (often optional info)\n    text = re.sub(r'\\([^)]*\\)', '', text)\n    \n    # Step 3: Remove punctuation and standardize spacing\n    text = re.sub(r'[^\\w\\s]', ' ', text)\n    \n    # Step 4: Standardize storage notation (e.g., 128GB, 128 GB -&gt; 128gb)\n    text = re.sub(r'(\\d+)\\s*gb', r'\\1gb', text)\n    \n    # Step 5: Remove common filler words that don't help identify products\n    text = re.sub(r'\\b(smartphone|phone|color|unlocked)\\b', '', text)\n    \n    # Step 6: Normalize whitespace for clean comparison\n    text = ' '.join(text.split())\n    \n    return text\n\n\n# Normalize all products\nnormalized_products = [normalize_product_text(p) for p in products]\n\nprint(\"Original vs. Normalized:\")\nfor original, normalized in zip(products, normalized_products):\n    print(f\"Original: {original}\")\n    print(f\"Normalized: {normalized}\\n\")\n\nOriginal vs. Normalized:\nOriginal: Samsung Galaxy S21 5G 128GB Smartphone (Unlocked)\nNormalized: samsung galaxy s21 5g 128gb\n\nOriginal: Samsung Galaxy S21 Ultra 256GB 5G Phone\nNormalized: samsung galaxy s21 ultra 256gb 5g\n\nOriginal: Samsung S21 5G 128 GB\nNormalized: samsung s21 5g 128gb\n\nOriginal: iPhone 13 Pro Max 256GB - Graphite\nNormalized: iphone 13 pro max 256gb graphite\n\nOriginal: Apple iPhone 13 Pro Max (256 GB, Graphite)\nNormalized: apple iphone 13 pro max\n\nOriginal: iPhone13 Pro 256GB\nNormalized: iphone13 pro 256gb\n\nOriginal: Google Pixel 6 Pro 128GB - Stormy Black\nNormalized: google pixel 6 pro 128gb stormy black\n\nOriginal: Pixel 6 Pro (128 GB) Stormy Black color\nNormalized: pixel 6 pro stormy black\n\n\n\n\n\nStep 2: Group Similar Products Using Fuzzy Matching\n\n# Group similar products\ndef group_similar_products(products, normalized_products, threshold=80):\n    \"\"\"Group similar products based on fuzzy matching\"\"\"\n    groups = []\n    used_indices = set()\n    \n    # Step 1: Iterate through each product\n    for i, norm_p1 in enumerate(normalized_products):\n        if i in used_indices:\n            continue\n            \n        # Step 2: Start a new group with this product\n        group = [products[i]]\n        used_indices.add(i)\n        \n        # Step 3: Find similar products by comparing with all others\n        for j, norm_p2 in enumerate(normalized_products):\n            if j != i and j not in used_indices:\n                # Step 4: Use token_set_ratio to handle different word orders\n                # and partial matches (this is key for product description matching!)\n                similarity = fuzz.token_set_ratio(norm_p1, norm_p2)\n                \n                # Step 5: If similarity exceeds our threshold, consider it a match\n                if similarity &gt;= threshold:\n                    group.append(products[j])\n                    used_indices.add(j)\n        \n        # Step 6: Add this group to our collection\n        groups.append(group)\n    \n    return groups\n\n\n# Group similar products\nsimilar_groups = group_similar_products(products, normalized_products)\n\nprint(\"Similar Product Groups:\")\nfor i, group in enumerate(similar_groups, 1):\n    print(f\"\\nGroup {i}:\")\n    for product in group:\n        print(f\"  - {product}\")\n\nSimilar Product Groups:\n\nGroup 1:\n  - Samsung Galaxy S21 5G 128GB Smartphone (Unlocked)\n  - Samsung Galaxy S21 Ultra 256GB 5G Phone\n  - Samsung S21 5G 128 GB\n\nGroup 2:\n  - iPhone 13 Pro Max 256GB - Graphite\n  - Apple iPhone 13 Pro Max (256 GB, Graphite)\n\nGroup 3:\n  - iPhone13 Pro 256GB\n\nGroup 4:\n  - Google Pixel 6 Pro 128GB - Stormy Black\n  - Pixel 6 Pro (128 GB) Stormy Black color\n\n\n\n\nKey Problem-Solving Takeaways\n\nBreak complex problems into steps: We separated normalization from grouping\nPreprocess data before comparison: Normalization makes matching more effective\nChoose appropriate algorithms: token_set_ratio works better than basic string comparison for product descriptions\nUse parameters to control behavior: The threshold value lets us adjust how strict the matching is\nManage already-processed data: The used_indices set prevents duplicate grouping\n\nTry modifying the threshold parameter to see how it affects the grouping results!",
    "crumbs": [
      "Combined Techniques: Putting It All Together"
    ]
  },
  {
    "objectID": "pages/999-contributors.html",
    "href": "pages/999-contributors.html",
    "title": "Contributors",
    "section": "",
    "text": "This course was written by Fahd Abdelazim with the support of Huw Day.\nThe course is maintained by the Jean Golding Institute.",
    "crumbs": [
      "Contributors"
    ]
  },
  {
    "objectID": "pages/100-strings.html",
    "href": "pages/100-strings.html",
    "title": "Python Strings: Beyond the Basics",
    "section": "",
    "text": "Let’s start with a quick refresher on some powerful string methods you may not use every day. For comprehensive documentation on Python strings, see the Python String Documentation.\n\n# String Methods Review\nsample_text = \"The quick brown fox jumps over the lazy dog\"\n\n# Splitting and joining\nwords = sample_text.split() #The split method divides a string into a list of substrings based on a specified separator (space by default)\nprint(\"Split into words:\", words)\nprint(\"Joined with dashes:\", \"-\".join(words)) #The join method combines a list of strings into one string, with a separator between each item.\n\nSplit into words: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\nJoined with dashes: The-quick-brown-fox-jumps-over-the-lazy-dog\n\n\n\n# Case manipulation\nprint(\"\\nCase manipulation:\")\nprint(\"Title case:\", sample_text.title()) #The title method converts the first character of each word to uppercase and the remaining characters to lowercase.\nprint(\"Swapped case:\", sample_text.swapcase()) #The swapcase method returns a copy of the string with uppercase characters converted to lowercase and lowercase characters converted to uppercase.\n\n\nCase manipulation:\nTitle case: The Quick Brown Fox Jumps Over The Lazy Dog\nSwapped case: tHE QUICK BROWN FOX JUMPS OVER THE LAZY DOG\n\n\n\n# Finding and counting\nprint(\"\\nFinding and counting:\")\nprint(\"'the' appears at index:\", sample_text.lower().find(\"the\")) #The find method tells you where a substring appears in a string, or returns -1 if it isn't there.\nprint(\"'the' appears\", sample_text.lower().count(\"the\"), \"times\") #The count method returns the number of times a specified substring appears in the string\n\n\nFinding and counting:\n'the' appears at index: 0\n'the' appears 2 times\n\n\n\n# Stripping and replacing\npadded_text = \"   \\t  spaces around   \\n\"\nprint(\"\\nStripped text:\", padded_text.strip()) #The strip method removes specified characters (spaces by default) from the beginning and end of a string.\nprint(\"Replacing:\", sample_text.replace(\"fox\", \"cat\")) #The replace method returns a new string where specified occurrences of an old substring are replaced with a new substring.\n\n\nStripped text: spaces around\nReplacing: The quick brown cat jumps over the lazy dog\n\n\n\nPractical Example: Text Preprocessing\nLet’s look at a common task: preprocessing text for analysis. We’ll break this down into steps:\n\nConvert text to lowercase to ensure consistency\nRemove punctuation that might interfere with word analysis\nNormalize whitespace to standardize the text\n\n\ndef preprocess_text(text):\n    \"\"\"Simple preprocessing function for text analysis\"\"\"\n    # Convert to lowercase\n    text = text.lower()\n    \n    # Remove punctuation (simple approach)\n    for char in \".,!?;:()[]{}'\\\"\\\\_-\":\n        text = text.replace(char, \" \")\n    \n    # Normalize whitespace\n    text = \" \".join(text.split())\n    \n    return text\n\nsample = \"Hello, world! This is a sample text... with some punctuation.\"\npreprocessed = preprocess_text(sample)\nprint(\"Original:\", sample)\nprint(\"Preprocessed:\", preprocessed)\n\nOriginal: Hello, world! This is a sample text... with some punctuation.\nPreprocessed: hello world this is a sample text with some punctuation\n\n\n\n\nF-strings and Formatting\nF-strings (introduced in Python 3.6) make string formatting much more readable. The syntax f\"...{variable}...\" allows embedding expressions inside string literals.\nFor more details, see the Python f-string documentation.\n\nname = \"Python\"\nversion = 3.11\nyears = 32\n\n# F-string with expressions and formatting\nprint(f\"{name} {version} is a powerful language that's {years} years old!\")\nprint(f\"Half of {years} is {years/2:.1f}\")  # :.1f formats to 1 decimal place\n\n# Alignment and padding\nfor i in range(1, 11):\n    # :2d means format as decimal integer with width 2\n    print(f\"{i:2d} squared is {i**2:3d} and cubed is {i**3:4d}\")\n\nPython 3.11 is a powerful language that's 32 years old!\nHalf of 32 is 16.0\n 1 squared is   1 and cubed is    1\n 2 squared is   4 and cubed is    8\n 3 squared is   9 and cubed is   27\n 4 squared is  16 and cubed is   64\n 5 squared is  25 and cubed is  125\n 6 squared is  36 and cubed is  216\n 7 squared is  49 and cubed is  343\n 8 squared is  64 and cubed is  512\n 9 squared is  81 and cubed is  729\n10 squared is 100 and cubed is 1000",
    "crumbs": [
      "Python Strings: Beyond the Basics"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "This short course provides a concise introduction to working with:\n\nPython strings and their methods\nRegular expressions for pattern matching\nFuzzy matching for approximate string matching\n\nBy the end of this 1-hour session, you’ll have a practical understanding of these techniques and when to apply them.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "pages/990-summary.html",
    "href": "pages/990-summary.html",
    "title": "Summary and Best Practices",
    "section": "",
    "text": "When to use each technique:\n\nString Methods: Best for simple operations on well-structured text\n\nSplitting, joining, case changes, simple replacements\nFast and built-in to Python\n\nRegular Expressions: Best for pattern matching and extraction\n\nWhen you need to find or replace text that follows specific patterns\nExtracting structured data from text\nValidating formats (emails, phone numbers, etc.)\n\nFuzzy Matching: Best for comparing similar but not identical strings\n\nHandling user input with typos or variations\nDeduplicating data where entries might differ slightly\nSearching for approximate matches\n\n\n\n\nPerformance Considerations:\n\nString methods are fastest\nRegex is powerful but can be slower for complex patterns\nFuzzy matching is the most computationally intensive\nPrecompile regex patterns when reusing them\nNormalize strings before comparison to improve match quality\n\n\n\nFurther Learning Resources:\n\nPython String Documentation\nPython Regex Documentation\nRegex101 - Interactive regex tester\nFuzzyWuzzy Documentation",
    "crumbs": [
      "Summary and Best Practices"
    ]
  },
  {
    "objectID": "pages/200-regex.html",
    "href": "pages/200-regex.html",
    "title": "Regular Expressions (Regex)",
    "section": "",
    "text": "Regular expressions allow for powerful pattern matching within strings. Python’s re module provides regex functionality.\n\nRegex Reference\nHere’s a quick reference of common regex patterns:\n\n\n\nPattern\nDescription\n\n\n\n\n\\d\nDigit (0-9)\n\n\n\\w\nWord character (a-z, A-Z, 0-9, _)\n\n\n\\s\nWhitespace\n\n\n\\b\nWord boundary\n\n\n[abc]\nAny character in brackets\n\n\n[^abc]\nAny character not in brackets\n\n\na*\nZero or more ‘a’\n\n\na+\nOne or more ‘a’\n\n\na?\nZero or one ‘a’\n\n\na{3}\nExactly 3 ‘a’\n\n\na{2,4}\n2 to 4 ‘a’\n\n\n()\nCapture group\n\n\na|b\nMatch ‘a’ or ‘b’\n\n\n\nFor the full regex documentation, see the Python Regex Documentation and try out patterns at Regex101 - an interactive regex tester.\n\nimport re\n\n# Basic pattern matching\ntext = \"The rainfall was 25.5 mm on Monday and 32.7 mm on Tuesday.\"\n\n# Find all numbers (including decimals)\n# \\d+ matches one or more digits\n# \\.? matches an optional decimal point\n# \\d* matches zero or more digits after the decimal point\nnumbers = re.findall(r'\\d+\\.?\\d*', text)\nprint(\"Numbers found:\", numbers)\n\n# Find all words that start with capital letters\n# \\b is a word boundary\n# [A-Z] matches any capital letter\n# [a-z]* matches zero or more lowercase letters\n# \\b is another word boundary to end the word\ncapitals = re.findall(r'\\b[A-Z][a-z]*\\b', text)\nprint(\"Capitalized words:\", capitals)\n\nNumbers found: ['25.5', '32.7']\nCapitalized words: ['The', 'Monday', 'Tuesday']\n\n\nRegex are easy to get wrong, so it’s good to write little test cases where you know what the answer should be to make sure your code is working correctly:\n\ntesttext = \"Here are Some words some Start With capital letters\"\ncapitals = re.findall(r'\\b[A-Z][a-z]*\\b', testtext)\nassert capitals == [\"Here\", \"Some\", \"Start\", \"With\"], \"Error your output should have been \" + str(capitals)\n\n\n\nRegex Patterns and Methods\nLet’s explore more regex functionality with practical examples. The re module provides several key functions:\n\nre.search(): Finds the first match of a pattern\nre.findall(): Finds all non-overlapping matches\nre.sub(): Substitutes matching patterns with a replacement\nre.compile(): Compiles a regex pattern for reuse\nre.finditer(): Returns an iterator of match objects\n\nSee Python’s re module documentation for more details.\n\n# Common regex functions: search, match, findall, sub\n\n# Email pattern explanation:\n# [\\w\\.-]+ - one or more word characters, dots, or hyphens (username)\n# @ - literal @ symbol\n# [\\w\\.-]+ - one or more word characters, dots, or hyphens (domain name)\n# \\. - literal dot\n# [a-zA-Z]{2,} - two or more letters (top-level domain)\nemail_pattern = r'[\\w\\.-]+@[\\w\\.-]+\\.[a-zA-Z]{2,}'\ntext = \"Contact us at info@example.com or support@company.co.uk for assistance.\"\n\n# search finds first match\nmatch = re.search(email_pattern, text)\nif match:\n    print(f\"First email found: {match.group()} at position {match.start()}\")\n\n# findall gets all matches\nemails = re.findall(email_pattern, text)\nprint(f\"All emails: {emails}\")\n\n# sub replaces matches\nredacted = re.sub(email_pattern, \"[EMAIL REDACTED]\", text)\nprint(f\"Redacted text: {redacted}\")\n\n# Capturing groups with ()\ndate_text = \"Important dates: 2023-04-15, 2023-05-22, and 2023-06-30.\"\n\n# Date pattern breakdown:\n# (\\d{4}) - capturing group for 4 digits (year)\n# - - literal hyphen\n# (\\d{2}) - capturing group for 2 digits (month)\n# - - literal hyphen\n# (\\d{2}) - capturing group for 2 digits (day)\ndate_pattern = r'(\\d{4})-(\\d{2})-(\\d{2})'\n\nfor match in re.finditer(date_pattern, date_text):\n    print(f\"Full date: {match.group(0)}\")\n    print(f\"  Year: {match.group(1)}\")\n    print(f\"  Month: {match.group(2)}\")\n    print(f\"  Day: {match.group(3)}\")\n\nFirst email found: info@example.com at position 14\nAll emails: ['info@example.com', 'support@company.co.uk']\nRedacted text: Contact us at [EMAIL REDACTED] or [EMAIL REDACTED] for assistance.\nFull date: 2023-04-15\n  Year: 2023\n  Month: 04\n  Day: 15\nFull date: 2023-05-22\n  Year: 2023\n  Month: 05\n  Day: 22\nFull date: 2023-06-30\n  Year: 2023\n  Month: 06\n  Day: 30\n\n\n\n\nPractical Example: Data Extraction\nProblem Statement: We need to extract error messages from a log file and analyze the frequency of different log types.\nApproach: 1. Define regex patterns to match log entries 2. Use capturing groups to extract relevant information (timestamp, message) 3. Analyze the extracted data\n\nlog_data = \"\"\"\n2023-06-15 08:24:32 INFO Server started successfully\n2023-06-15 08:25:47 WARNING Disk space at 85%\n2023-06-15 08:30:15 ERROR Failed to connect to database: timeout\n2023-06-15 08:35:22 INFO User admin logged in\n2023-06-15 08:40:13 ERROR Authentication failed for user guest\n\"\"\"\n\n# Extract error messages\n# Pattern breakdown:\n# (\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}) - capture the timestamp\n# ERROR - match the ERROR log level\n# (.*) - capture the rest of the line (the error message)\nerror_pattern = r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}) ERROR (.*)'\nerrors = re.findall(error_pattern, log_data)\n\nprint(\"Error log entries:\")\nfor timestamp, message in errors:\n    print(f\"[{timestamp}] {message}\")\n\n# Count log types\n# Pattern breakdown:\n# space followed by INFO, WARNING, or ERROR\nlog_types = re.findall(r' (INFO|WARNING|ERROR) ', log_data)\nprint(\"\\nLog type counts:\")\nfor log_type in set(log_types):\n    count = log_types.count(log_type)\n    print(f\"{log_type}: {count}\")\n\nError log entries:\n[2023-06-15 08:30:15] Failed to connect to database: timeout\n[2023-06-15 08:40:13] Authentication failed for user guest\n\nLog type counts:\nINFO: 2\nWARNING: 1\nERROR: 2",
    "crumbs": [
      "Regular Expressions (Regex)"
    ]
  },
  {
    "objectID": "pages/300-fuzzy-matching.html",
    "href": "pages/300-fuzzy-matching.html",
    "title": "Fuzzy String Matching",
    "section": "",
    "text": "Sometimes we need to find approximate matches rather than exact patterns. This is where fuzzy matching comes in.\nLet’s install the necessary library first. It’s recommended to do this in a virtual environment to keep your dependencies isolated.\n:::",
    "crumbs": [
      "Fuzzy String Matching"
    ]
  },
  {
    "objectID": "pages/300-fuzzy-matching.html#levenshtein-distance",
    "href": "pages/300-fuzzy-matching.html#levenshtein-distance",
    "title": "Fuzzy String Matching",
    "section": "Levenshtein Distance",
    "text": "Levenshtein Distance\nAt its core, it utilizes the Levenshtein distance, which measures the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one string into the other.  \n\nSimilarity Score: The ratio() method then translates this distance into a similarity score, expressed as a percentage between 0 and 100. A score of 100 indicates a perfect match.\nPurpose: It’s designed to handle situations where strings might not be exactly identical due to typos, variations in spelling, or other minor differences.\nHow it works: Essentially it takes the two strings, and then calculates the Levenshtein distance. That distance is then used in a calculation to return a number between 0 and 100, that represents how similar the two strings are.\n\nIn essence, fuzzywuzzy.ratio() provides a quick and effective way to quantify the likeness of two strings.\n\n# Simple ratio - Character-based similarity\nprint(f\"How similar are `{s1}` and `{s2}`?\")\nprint(f\"Simple ratio: {fuzz.ratio(s1.lower(), s2.lower())}\")\n\nHow similar are `Python Programming` and `programming python`?\nSimple ratio: 61\n\n\nThe fuzzywuzzy.token_sort_ratio() method is another similarity scoring function within the FuzzyWuzzy library, designed to handle cases where strings have the same words but in a different order.",
    "crumbs": [
      "Fuzzy String Matching"
    ]
  },
  {
    "objectID": "pages/300-fuzzy-matching.html#tokenization",
    "href": "pages/300-fuzzy-matching.html#tokenization",
    "title": "Fuzzy String Matching",
    "section": "Tokenization",
    "text": "Tokenization\nFirst, it splits the input strings into individual words (tokens). # Sorting Then, it sorts these tokens alphabetically. # String Reconstruction The sorted tokens are then joined back into strings. # Ratio Calculation Finally, it calculates the simple ratio() between these sorted strings, effectively ignoring word order.",
    "crumbs": [
      "Fuzzy String Matching"
    ]
  },
  {
    "objectID": "pages/300-fuzzy-matching.html#key-differences-from-ratio",
    "href": "pages/300-fuzzy-matching.html#key-differences-from-ratio",
    "title": "Fuzzy String Matching",
    "section": "Key Differences from ratio()",
    "text": "Key Differences from ratio()\n\nratio() is sensitive to word order.\ntoken_sort_ratio() is insensitive to word order.",
    "crumbs": [
      "Fuzzy String Matching"
    ]
  },
  {
    "objectID": "pages/300-fuzzy-matching.html#use-case",
    "href": "pages/300-fuzzy-matching.html#use-case",
    "title": "Fuzzy String Matching",
    "section": "Use Case",
    "text": "Use Case\nThis method is particularly useful when comparing strings where the word order might vary, such as addresses, names with titles, or phrases with slight rearrangements.",
    "crumbs": [
      "Fuzzy String Matching"
    ]
  },
  {
    "objectID": "pages/300-fuzzy-matching.html#example",
    "href": "pages/300-fuzzy-matching.html#example",
    "title": "Fuzzy String Matching",
    "section": "Example",
    "text": "Example\n“New York Mets” and “Mets New York” would have a low ratio() score because of the different word order. However, they would have a perfect token_sort_ratio() score of 100 because the words are the same, just in a different order.\nIn summary, token_sort_ratio() prioritizes the presence of the same words, regardless of their sequence, by sorting the tokens before calculating the similarity.\n\n# Token sort ratio - Word-based similarity after sorting\nprint(f\"Token sort ratio: {fuzz.token_sort_ratio(s1.lower(), s2.lower())}\")\n\nToken sort ratio: 100\n\n\nThe fuzzywuzzy.partial_ratio() method is designed to find the best partial match between two strings.",
    "crumbs": [
      "Fuzzy String Matching"
    ]
  },
  {
    "objectID": "pages/300-fuzzy-matching.html#partial-matching",
    "href": "pages/300-fuzzy-matching.html#partial-matching",
    "title": "Fuzzy String Matching",
    "section": "Partial Matching",
    "text": "Partial Matching\nUnlike fuzz.ratio(), which compares the entire strings, partial_ratio() looks for the best substring match. This is particularly useful when one string is much longer than the other, and you’re interested in whether the shorter string exists within the longer one.",
    "crumbs": [
      "Fuzzy String Matching"
    ]
  },
  {
    "objectID": "pages/300-fuzzy-matching.html#substring-comparison",
    "href": "pages/300-fuzzy-matching.html#substring-comparison",
    "title": "Fuzzy String Matching",
    "section": "Substring Comparison",
    "text": "Substring Comparison\nIt essentially takes the shorter string and compares it to all possible substrings of the same length within the longer string. It then returns the highest similarity score found among those substring comparisons.",
    "crumbs": [
      "Fuzzy String Matching"
    ]
  },
  {
    "objectID": "pages/300-fuzzy-matching.html#use-cases",
    "href": "pages/300-fuzzy-matching.html#use-cases",
    "title": "Fuzzy String Matching",
    "section": "Use Cases",
    "text": "Use Cases\nThis is very helpful in situations like: Comparing a short address snippet to a full address. Finding a name within a longer text string. Searching for a product name inside of a much longer product description.",
    "crumbs": [
      "Fuzzy String Matching"
    ]
  },
  {
    "objectID": "pages/300-fuzzy-matching.html#key-points",
    "href": "pages/300-fuzzy-matching.html#key-points",
    "title": "Fuzzy String Matching",
    "section": "Key points",
    "text": "Key points\nIt focuses on finding if a smaller string is contained inside of a larger string. It returns the highest ratio of similarity found when comparing the smaller string to all possible substrings of the larger string.\nIn essence, partial_ratio() helps identify partial string matches, making it robust for cases where exact string equality is not expected.\n\n# Partial ratio - Best substring match\nprint(f\"Partial ratio: {fuzz.partial_ratio(s1.lower(), s2.lower())}\")\n\nPartial ratio: 61\n\n\n\nprint(\"\\nComparing multiple strings to 'Python Programming':\")\nfor s in [s2, s3, s4]:\n    print(f\"{s!r}: {fuzz.ratio(s1.lower(), s.lower())}\")\n\n\nComparing multiple strings to 'Python Programming':\n'programming python': 61\n'Pythn Programing': 94\n'Java Programming': 71\n\n\n\nFinding Best Matches in a List\nProblem: We need to find the closest matches to user input that may contain typos.\nApproach:\n\nDefine a list of correct terms (fruits in this case)\nFor each user query (potentially with typos), find the closest matches\nRank matches by similarity score\n\n\n# Example: Finding the closest match in a list\nfruits = [\"apple\", \"banana\", \"orange\", \"strawberry\", \"blueberry\", \"blackberry\"]\n\n# Sample user inputs with typos\nqueries = [\"aple\", \"banan\", \"blckberry\", \"strawbery\"]\n\nfor query in queries:\n    print(f\"Query: {query}\")\n    \n    # Get top 2 matches with scores using process.extract\n    # This function takes:\n    # - The query string\n    # - A list of choices to match against\n    # - limit: number of results to return\n    matches = process.extract(query, fruits, limit=2)\n    for match, score in matches:\n        print(f\"  Match: {match}, Score: {score}\")\n    \n    # Get the single best match using process.extractOne\n    # Returns a tuple of (best_match, score)\n    best_match = process.extractOne(query, fruits)\n    print(f\"  Best match: {best_match[0]} (Score: {best_match[1]})\\n\")\n\nQuery: aple\n  Match: apple, Score: 89\n  Match: orange, Score: 45\n  Best match: apple (Score: 89)\n\nQuery: banan\n  Match: banana, Score: 91\n  Match: orange, Score: 36\n  Best match: banana (Score: 91)\n\nQuery: blckberry\n  Match: blackberry, Score: 95\n  Match: blueberry, Score: 78\n  Best match: blackberry (Score: 95)\n\nQuery: strawbery\n  Match: strawberry, Score: 95\n  Match: blackberry, Score: 53\n  Best match: strawberry (Score: 95)\n\n\n\n\n\nPractical Example: Deduplicating User Entry Data\nProblem Statement: We have a list of city names entered by users that may contain duplicates with slight variations (e.g., “New York” vs “NYC”). We need to identify potential duplicates.\nApproach:\n\nDefine a function to detect similar items using fuzzy matching\nCompare each pair of cities and calculate similarity\nIdentify pairs with similarity above a certain threshold\n\n\n# Example: Deduplicating user-entered data\ncities = [\n    \"New York\", \"New York City\", \"NYC\", \"San Francisco\", \"San Fransisco\",\n    \"Sanfran\", \"Los Angeles\", \"LA\", \"Los Angles\", \"Chicago\"\n]\n\ndef find_duplicates(items, threshold=80):\n    \"\"\"Find potential duplicates in a list based on fuzzy matching\n    \n    Args:\n        items: List of strings to check for duplicates\n        threshold: Minimum similarity score (0-100) to consider as duplicate\n        \n    Returns:\n        List of tuples (item1, item2, similarity_score)\n    \"\"\"\n    duplicates = []\n    \n    # Check each pair of items\n    for i, item1 in enumerate(items):\n        for j, item2 in enumerate(items[i+1:], i+1):\n            # Use token_sort_ratio for better matching of different word orders\n            similarity = fuzz.token_sort_ratio(item1.lower(), item2.lower())\n            if similarity &gt;= threshold:\n                duplicates.append((item1, item2, similarity))\n    \n    return duplicates\n\n# Find potential duplicates\npotential_dupes = find_duplicates(cities)\nprint(\"Potential duplicate cities:\")\nfor item1, item2, score in potential_dupes:\n    print(f\"{item1} ⟷ {item2}: {score}%\")\n\nPotential duplicate cities:\nSan Francisco ⟷ San Fransisco: 92%\nLos Angeles ⟷ Los Angles: 95%",
    "crumbs": [
      "Fuzzy String Matching"
    ]
  }
]